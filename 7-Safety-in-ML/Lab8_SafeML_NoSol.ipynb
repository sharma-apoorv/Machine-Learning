{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-astronomy",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr style=\"background-color:#FFFFFF;\">\n",
    "<td width=20%>\n",
    "    <table>\n",
    "        <tr><img style=\"text-align:left\" src=\"./img/logo_uga.jpeg\" width=\"80\" ></br></a>\n",
    "        </tr> \n",
    "    </table>\n",
    "</td>\n",
    "<td><center><h1>Lab 8 : Safe Machine Learning via Superquantile Optimization</h1></center></td>\n",
    "<td width=15%>\n",
    "    <table>\n",
    "        <tr><img src=\"./img/logo_uw.png\" width=\"300\"></br></a>\n",
    "        </tr>\n",
    "    </table>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">\n",
    "    <table>\n",
    "            <tr><a href=\"https://yassine-laguel.github.io\" style=\"font-size: 15px \">Yassine Laguel <br></a></tr>\n",
    "    </table>       \n",
    "<td style=\"text-align:center\">DATA 558, Spring 2021</td>\n",
    "<td style=\"text-align:right\">\n",
    "    <table>\n",
    "    <tr> <a href=\"https://ronakdm.github.io\" style=\"font-size: 15px\">Ronak Mehta<br></a> </tr>\n",
    "    <tr> <a href=\"https://sites.stat.washington.edu/people/alecgt/\" style=\"font-size: 15px\">Alec Greaves-Tunnell <br></a> </tr>\n",
    "    <tr> <a href=\"https://krishnap25.github.io\" style=\"font-size: 15px\">Krishna Pillutla </br></a> </tr>\n",
    "    </table>\n",
    "    </td> \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capital-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-newman",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lab, we will:\n",
    "- Discuss metrics to quantify robustness for both classification and regression.\n",
    "- Investigate these metrics on two toy problems.\n",
    "- Learn how to train superquantile-based models for classification and regression.\n",
    "- Quantify and visualize the impact of these models on robustness.\n",
    "- Apply these tools to real datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-beast",
   "metadata": {},
   "source": [
    "## 1. Motivations\n",
    "\n",
    "Distributional robustness can be measured in two ways:\n",
    "\n",
    "- Better performance for worst-case outcomes.\n",
    "- Better performances for pessimistic distributional shifts.\n",
    "\n",
    "### 1.1 Illustration on a regression task\n",
    "\n",
    "Let us consider a linear regression task on the following synthetic training dataset:\n",
    "\n",
    "$$\n",
    "    y_i = \\alpha x_i  + \\beta + \\varepsilon_i \\quad \\text{ with } \\varepsilon_i = \\beta\\; \\varepsilon_{1} + (1-\\beta) \\varepsilon_{\n",
    "    2}\n",
    "$$\n",
    "\n",
    "where \n",
    "- $x_i \\in \\mathbb{R}$ is sampled from a uniform distribution on $[0,100]$.\n",
    "- $\\varepsilon_{1}$ and $\\varepsilon_{2}$ follow a normal law with respective parameters $(\\mu_1, \\sigma_1) = (0, 1)$ and $(\\mu_2, \\sigma_2) = (10, 3)$.\n",
    "- $\\beta$ follows a Bernouilli distribution with parameter $p=0.8$.\n",
    "- $\\alpha = 0.2,\\; \\beta=1.$\n",
    "\n",
    "Let's visualize our dataset together with performance of a classical linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "annual-economics",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_regression_data(seed=1, size=300):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    beta = 1.0\n",
    "    alpha = 0.2\n",
    "    x = np.random.uniform(high=100,size=size)\n",
    "    eps1 = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    eps2 = np.random.normal(loc=10.0, scale=5.0, size=size)\n",
    "    beta = np.random.binomial(1, 0.7, size=size)\n",
    "    \n",
    "    noise = beta * eps1 + (1-beta) * eps2 \n",
    "    y = beta * np.ones(size) + alpha * x + noise\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def plot_synthetic_regression(x, y):\n",
    "    \n",
    "    formatted_x = x.reshape(-1, 1)\n",
    "    lr = LinearRegression().fit(formatted_x, y)\n",
    "    errors = np.abs(y - lr.predict(formatted_x))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "    graph = axes[0]\n",
    "    graph.title.set_text('Synthetic Regression dataset')\n",
    "    graph.scatter(x, y, alpha=0.3)\n",
    "    t = np.array([0, 100]).reshape(-1,1)\n",
    "    ft = lr.predict(t)\n",
    "    graph.plot(t, ft, color='g', label='Linear Regression')\n",
    "    graph.legend()\n",
    "         \n",
    "    hist = axes[1]\n",
    "    hist.title.set_text('Histogram of errors')\n",
    "    \n",
    "#     kwargs = dict(histtype='bar', ec='black', alpha=0.3, bins=bins)\n",
    "    weights = np.ones_like(errors)/len(errors)\n",
    "    hist.hist(errors, weights=weights, color = \"g\", bins=25, alpha=0.3, histtype='bar', ec='black')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "x, y = generate_regression_data(seed=1)\n",
    "plot_synthetic_regression(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-miller",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Comment the graphs. \n",
    "\n",
    "Let us compute a few metrics to evaluate the worst-case performances of our linear model.\n",
    "\n",
    "**Exercise 1.2**: Implement the quantile and superquantile functions respectively defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_p(U) &= \\inf\\{t \\in \\mathbb{R}, \\mathbb{P}[U \\leq t] \\geq p\\} \\\\\n",
    "\\text{and }\\;\\; \\bar{Q}_p(U) &= \\frac{1}{1-p} \\int_{p'=p}^1 Q_{p'}(U) dp'\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innovative-miami",
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-04d09a016319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuperquantile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquantile_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m### <YOUR_CODE_HERE>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquantile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/apoor/OneDrive/Documents/UW/2021-Spring/DATA-558_Machine_Learning/Labs/Lab8_SafeML/spqr/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_template\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplateClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_template\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplateTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOracleSubgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOracleSmoothGradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrisk_optimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRiskOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDRLinearRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDRLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/apoor/OneDrive/Documents/UW/2021-Spring/DATA-558_Machine_Learning/Labs/Lab8_SafeML/spqr/oracle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeasures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moracles_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/apoor/OneDrive/Documents/UW/2021-Spring/DATA-558_Machine_Learning/Labs/Lab8_SafeML/spqr/measures.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "from spqr.measures import quantile, superquantile\n",
    "\n",
    "def quantile_function(p, U):\n",
    "    v = np.sort(U)\n",
    "    n = len(U)\n",
    "    i = 0\n",
    "    while i/n < p:\n",
    "        i += 1\n",
    "    return v[i]\n",
    "\n",
    "def superquantile_function(p, U):\n",
    "    ### <YOUR_CODE_HERE>\n",
    "    superquantile = None\n",
    "    ###\n",
    "    \n",
    "    v = np.sort(U)\n",
    "    \n",
    "    \n",
    "    upper = ((quantile_function(p, U) - p) / (1-p)) * quantile_function(p, U)\n",
    "    lower = ((1 - quantile_function(p, U)) / (1-p)) * \n",
    "    \n",
    "    return superquantile\n",
    "\n",
    "# Answers\n",
    "##\n",
    "\n",
    "def plot_synthetic_regression_with_metrics(x, y):\n",
    "    \n",
    "    formatted_x = x.reshape(-1, 1)\n",
    "    lr = LinearRegression().fit(formatted_x, y)\n",
    "    errors = np.abs(y - lr.predict(formatted_x))\n",
    "    p=0.9\n",
    "    q_p = quantile_function(p, errors)\n",
    "    superq_p = superquantile_function(p, errors)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "    \n",
    "    graph = axes[0]\n",
    "    graph.title.set_text('Synthetic Regression dataset')\n",
    "    graph.scatter(x, y, alpha=0.3)\n",
    "    t = np.array([0, 100]).reshape(-1,1)\n",
    "    ft = lr.predict(t)\n",
    "    graph.plot(t, ft, color='g', label='Linear Regression')\n",
    "    graph.legend()\n",
    "         \n",
    "    hist = axes[1]\n",
    "    hist.title.set_text('Histogram of errors')\n",
    "    weights = np.ones_like(errors)/len(errors)\n",
    "    hist.hist(errors, weights=weights, color = \"g\", bins=25, alpha=0.3, histtype='bar', ec='black')\n",
    "    hist.axvline(q_p, linestyle='-', color='g', label=str(p) + '-quantile')\n",
    "    hist.axvline(superq_p, linestyle='-.', color='g', label=str(p) + '-superquantile')\n",
    "    \n",
    "    hist.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_synthetic_regression_with_metrics(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-bulletin",
   "metadata": {},
   "source": [
    "**Exercise 1.3**: Given the above distribution of errors, computes the following metrics and report them in a table:\n",
    "- the minimum value \n",
    "- the mean value\n",
    "- the $p$-quantile for $p\\in \\{0.0, 0.2, 0.5, 0.7, 0.9, 0.99\\} $ \n",
    "- the $p$-superquantile for $p\\in \\{0.0, 0.2, 0.5, 0.7, 0.9, 0.99\\}$ \n",
    "\n",
    "Comment the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <YOUR_CODE_HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-occupation",
   "metadata": {},
   "source": [
    "**Your comments:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-robinson",
   "metadata": {},
   "source": [
    "### 1.2 Illustration on a classification task\n",
    "\n",
    "Let us now consider a binary classification task on a real dataset: the Australian credit approcal [dataset](https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval) from the UCI Repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-graduation",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from course_utils import fetch_australian_dataset\n",
    "x_tr, y_tr, x_te, y_te = fetch_australian_dataset(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-national",
   "metadata": {},
   "source": [
    "**Exercice 1.3:** Evaluate first the proportion of each class for both the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <YOUR_CODE_HERE>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-price",
   "metadata": {},
   "source": [
    "A popular linear model to train for such tasks is the [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). It consists in minizing the loss:\n",
    "\n",
    "$$\n",
    "        \\min_{w, c} \\frac{1}{n} \\sum_{i=1}^{n} \\log(\\exp(-y_i(X_i^\\top w + c)) + 1) + \\frac{1}{nC} \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "Let us start by training a logistic regression model on the training sample and evaluate its performances on the training set and the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=False).fit(x_tr, y_tr)\n",
    "\n",
    "y_hat_tr = clf.predict(x_tr)\n",
    "y_hat_te = clf.predict(x_te)\n",
    "\n",
    "print(\"Train accuracy score : {acc:.3f}\".format(acc=accuracy_score(y_tr, y_hat_tr)))\n",
    "print(\"Train precision score : {acc:.3f}\".format(acc=precision_score(y_tr, y_hat_tr)))\n",
    "\n",
    "print(\"Test accuracy score : {acc:.3f}\".format(acc=accuracy_score(y_te, y_hat_te)))\n",
    "print(\"Test precision score : {acc:.3f}\".format(acc=precision_score(y_te, y_hat_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-weather",
   "metadata": {},
   "source": [
    "The model produced generalizes well on the observed testing set. We may now investigate performances of this model for a sequence of distributional shifts on the testing set. We implement a python function `shift` to rebalance arbitrarily the proportion of each class in the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_utils import get_data_class\n",
    "def shift(x, y, new_proportion, seed=1):\n",
    "    generator = np.random.default_rng(seed)\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    max_size = np.min(counts)\n",
    "    \n",
    "    x_0, y_0 = get_data_class(x, y, 0)\n",
    "    x_1, y_1 = get_data_class(x, y, 1)\n",
    "    subsampled_index_0 = generator.choice(np.arange(x_0.shape[0]), size=np.ceil(new_proportion * max_size).astype(np.int64),\n",
    "                                          replace=False)\n",
    "    subsampled_index_1 = generator.choice(np.arange(x_1.shape[0]), size=np.ceil((1.0-new_proportion) * max_size).astype(np.int64),\n",
    "                                          replace=False)\n",
    "    x_0 = x_0[subsampled_index_0]\n",
    "    y_0 = y_0[subsampled_index_0]\n",
    "    x_1 = x_1[subsampled_index_1]\n",
    "    y_1 = y_1[subsampled_index_1]\n",
    "    \n",
    "    new_x = np.concatenate([x_0, x_1])\n",
    "    new_y = np.concatenate([y_0, y_1])\n",
    "\n",
    "    shuffled_index = generator.permutation(np.arange(new_x.shape[0]))\n",
    "    new_x = new_x[shuffled_index]\n",
    "    new_y = new_y[shuffled_index]\n",
    "\n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-proxy",
   "metadata": {},
   "source": [
    "**Exercice 1.4:** Evaluate the performances of the linear model on several distributionnal shifts of the testing set. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <YOUR_CODE_HERE>\n",
    "\n",
    "# Solution\n",
    "# lst_prop = np.linspace(0,1, num=100)\n",
    "lst_prop = np.linspace(0,1.0, num=100)\n",
    "\n",
    "lst_accuracy = []\n",
    "lst_min_accuracy = []\n",
    "lst_max_accuracy = []\n",
    "\n",
    "lst_precision = []\n",
    "lst_min_precision = []\n",
    "lst_max_precision = []\n",
    "\n",
    "n_seeds = 30\n",
    "\n",
    "for prop in lst_prop:\n",
    "    temp_acc = []\n",
    "    temp_prec = []\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        shifted_x, shifted_y = shift(x_te, y_te, prop, seed=seed)\n",
    "        y_hat = clf.predict(shifted_x)\n",
    "        temp_acc.append(accuracy_score(shifted_y, y_hat))\n",
    "        temp_prec.append(precision_score(shifted_y, y_hat))\n",
    "    max_acc = np.mean(temp_acc) + np.std(temp_acc)\n",
    "    min_acc = np.mean(temp_acc) - np.std(temp_acc)\n",
    "    mean_acc = np.mean(temp_acc)\n",
    "    max_prec = np.mean(temp_prec) + np.std(temp_prec)\n",
    "    min_prec = np.mean(temp_prec) - np.std(temp_prec)\n",
    "    mean_prec = np.mean(temp_prec)\n",
    "    \n",
    "    \n",
    "    lst_accuracy.append(mean_acc)\n",
    "    lst_min_accuracy.append(min_acc)\n",
    "    lst_max_accuracy.append(max_acc)\n",
    "\n",
    "    lst_precision.append(mean_prec)\n",
    "    lst_min_precision.append(min_prec)\n",
    "    lst_max_precision.append(max_prec)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "\n",
    "graph1 = axes[0]\n",
    "graph1.title.set_text('Accuracies')\n",
    "weights = np.ones_like(lst_accuracy)/len(lst_accuracy)\n",
    "graph1.plot(lst_prop, lst_accuracy, color = \"g\", alpha=1.0)\n",
    "graph1.fill_between(lst_prop, lst_min_accuracy, lst_max_accuracy, color = \"g\", alpha=0.3)\n",
    "graph1.set_xlabel('shift proportion', fontsize=12)\n",
    "\n",
    "graph2 = axes[1]\n",
    "graph2.title.set_text('Precision')\n",
    "weights = np.ones_like(lst_precision)/len(lst_precision)\n",
    "graph2.plot(lst_prop, lst_precision, color = \"g\",  alpha=1.0)\n",
    "graph2.fill_between(lst_prop, lst_min_precision, lst_max_precision, color = \"g\", alpha=0.3)\n",
    "graph2.set_xlabel('shift proportion', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-macintosh",
   "metadata": {},
   "source": [
    "## 2. The toolbox `spqr`\n",
    "\n",
    "`sqpr` is a python toolbox, built on top of `sklearn` and aimed at providing optimization first order convex algorithms for the minimization of superquantile based measures. For more information we refer to its [online documentation](https://yassine-laguel.github.io/spqr/).\n",
    "\n",
    "In this tutorial, we'll focus on how to use `SPQR` to perform distributionally robust regression and classification tasks. For that purpose, the toolbox provides users with two estimators:\n",
    "- `DRLinearRegression`, for regression, which solves:\n",
    "\n",
    "$$\n",
    "\\min_{w, c} \\bar{Q}_p((Y - w_\\top X - c)) + \\frac{1.}{nC} \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "- `DRLogisticRegression` which solves:\n",
    "$$\n",
    "\\min_{w, c} \\bar{Q}_p(\\log(\\exp(-Y(X^\\top w + c)) + 1)) + \\frac{1.}{nC} \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "### 2.1 $\\ell_2$-regression with `spqr`\n",
    "\n",
    "Let's instantiate a `DRLinearRegression` object to fit our previous regression dataset. \n",
    "\n",
    "**Exercise 2.1:** Implement a distributionally robust linear regression on the previous regression dataset and compare its performance against the classical linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spqr import DRLinearRegression\n",
    "\n",
    "def plot_synthetic_regression_with_metrics(x, y):\n",
    "    \n",
    "    formatted_x = x.reshape(-1, 1)\n",
    "    lr = LinearRegression().fit(formatted_x, y)\n",
    "    errors = np.abs(y - lr.predict(formatted_x))\n",
    "    p=0.9\n",
    "    q_p = quantile_function(p, errors)\n",
    "    superq_p = superquantile_function(p, errors)\n",
    "    \n",
    "    regressor = DRLinearRegression(p=0.9, fit_intercept=True)\n",
    "    regressor.fit(formatted_x, y)\n",
    "    errors2 = np.abs(y - regressor.predict(formatted_x).ravel())\n",
    "    p=0.9\n",
    "    q_p2 = quantile_function(p, errors2)\n",
    "    superq_p2 = superquantile_function(p, errors2)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "    \n",
    "    graph = axes[0]\n",
    "    graph.title.set_text('Synthetic Regression dataset')\n",
    "    graph.scatter(x, y, alpha=0.3)\n",
    "    t = np.array([0, 100]).reshape(-1,1)\n",
    "    ft = lr.predict(t)\n",
    "    ft2 = regressor.predict(t)\n",
    "    graph.plot(t, ft, color='g', label='Linear Regression')\n",
    "    graph.plot(t, ft2, color='r', label='Superquantile Linear Regression')\n",
    "    graph.legend()\n",
    "         \n",
    "    hist = axes[1]\n",
    "    hist.title.set_text('Histogram of errors')\n",
    "    weights = np.ones_like(errors)/len(errors)\n",
    "    weights2 = np.ones_like(errors2)/len(errors2)\n",
    "    hist.hist(errors, weights=weights, color = \"g\", bins=25, alpha=0.3, histtype='bar', ec='black')\n",
    "    hist.hist(errors2, weights=weights2, color = \"r\", bins=25, alpha=0.3, histtype='bar', ec='black')\n",
    "    hist.axvline(q_p, linestyle='-', color='g', label=str(p) + '-quantile')\n",
    "    hist.axvline(superq_p, linestyle='-.', color='g', label=str(p) + '-superquantile')\n",
    "    hist.axvline(q_p2, linestyle='-', color='r', label=str(p) + '-quantile')\n",
    "    hist.axvline(superq_p2, linestyle='-', color='r', label=str(p) + '-superquantile')\n",
    "    \n",
    "    hist.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_regression_data(seed=1)\n",
    "plot_synthetic_regression_with_metrics(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-desktop",
   "metadata": {},
   "source": [
    "### 2.2 Logistic regression with `spqr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spqr import DRLogisticRegression\n",
    "\n",
    "\n",
    "super_clf = DRLogisticRegression(p=0.8, fit_intercept=False)\n",
    "super_clf.fit(x_tr, y_tr)\n",
    "\n",
    "y_hat_tr = super_clf.predict(x_tr)\n",
    "y_hat_te = super_clf.predict(x_te)\n",
    "\n",
    "print(\"Train accuracy score : {acc:.3f}\".format(acc=accuracy_score(y_tr, y_hat_tr)))\n",
    "print(\"Train precision score : {acc:.3f}\".format(acc=precision_score(y_tr, y_hat_tr)))\n",
    "\n",
    "print(\"Test accuracy score : {acc:.3f}\".format(acc=accuracy_score(y_te, y_hat_te)))\n",
    "print(\"Test precision score : {acc:.3f}\".format(acc=precision_score(y_te, y_hat_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-wilderness",
   "metadata": {},
   "source": [
    "**Exercise 2.1:** Compare the performances of the superquantile model with the risk-neutral model on a sequence of distributional shifts. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_prop = np.linspace(0.0,1.0, num=100)\n",
    "\n",
    "lst_accuracy = []\n",
    "lst_min_accuracy = []\n",
    "lst_max_accuracy = []\n",
    "\n",
    "lst_precision = []\n",
    "lst_min_precision = []\n",
    "lst_max_precision = []\n",
    "\n",
    "lst_accuracy2 = []\n",
    "lst_min_accuracy2 = []\n",
    "lst_max_accuracy2 = []\n",
    "\n",
    "lst_precision2 = []\n",
    "lst_min_precision2 = []\n",
    "lst_max_precision2 = []\n",
    "\n",
    "for prop in lst_prop:\n",
    "    temp_acc = []\n",
    "    temp_prec = []\n",
    "    \n",
    "    temp_acc2 = []\n",
    "    temp_prec2 = []\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        shifted_x, shifted_y = shift(x_te, y_te, prop, seed=seed)\n",
    "        y_hat = clf.predict(shifted_x)\n",
    "        y_hat2 = super_clf.predict(shifted_x)\n",
    "        temp_acc.append(accuracy_score(shifted_y, y_hat))\n",
    "        temp_prec.append(precision_score(shifted_y, y_hat))\n",
    "        temp_acc2.append(accuracy_score(shifted_y, y_hat2))\n",
    "        temp_prec2.append(precision_score(shifted_y, y_hat2))\n",
    "\n",
    "    max_acc = np.mean(temp_acc) + np.std(temp_acc)\n",
    "    min_acc = np.mean(temp_acc) - np.std(temp_acc)\n",
    "    mean_acc = np.mean(temp_acc)\n",
    "    max_prec = np.mean(temp_prec) + np.std(temp_prec)\n",
    "    min_prec = np.mean(temp_prec) - np.std(temp_prec)\n",
    "    mean_prec = np.mean(temp_prec)\n",
    "    \n",
    "\n",
    "    max_acc2 = np.mean(temp_acc2) + np.std(temp_acc2)\n",
    "    min_acc2 = np.mean(temp_acc2) - np.std(temp_acc2)\n",
    "    mean_acc2 = np.mean(temp_acc2)\n",
    "    max_prec2 = np.mean(temp_prec2) + np.std(temp_prec2)\n",
    "    min_prec2 = np.mean(temp_prec2) - np.std(temp_prec2)\n",
    "    mean_prec2 = np.mean(temp_prec2)\n",
    "    \n",
    "    lst_accuracy.append(mean_acc)\n",
    "    lst_min_accuracy.append(min_acc)\n",
    "    lst_max_accuracy.append(max_acc)\n",
    "    lst_precision.append(mean_prec)\n",
    "    lst_min_precision.append(min_prec)\n",
    "    lst_max_precision.append(max_prec)\n",
    "    \n",
    "    lst_accuracy2.append(mean_acc2)\n",
    "    lst_min_accuracy2.append(min_acc2)\n",
    "    lst_max_accuracy2.append(max_acc2)\n",
    "    lst_precision2.append(mean_prec2)\n",
    "    lst_min_precision2.append(min_prec2)\n",
    "    lst_max_precision2.append(max_prec2)\n",
    "    \n",
    "fig, axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "\n",
    "graph1 = axes[0]\n",
    "graph1.title.set_text('Accuracies')\n",
    "graph1.plot(lst_prop, lst_accuracy, color = \"g\", alpha=1.0, label='Logistic Regression')\n",
    "graph1.plot(lst_prop, lst_accuracy2, color = \"r\", alpha=1.0, label='Superquantile Logistic Regression')\n",
    "graph1.fill_between(lst_prop, lst_min_accuracy, lst_max_accuracy, color = \"g\", alpha=0.3)\n",
    "graph1.fill_between(lst_prop, lst_min_accuracy2, lst_max_accuracy2, color = \"r\", alpha=0.3)\n",
    "graph1.set_xlabel('shift proportion', fontsize=12)\n",
    "graph1.legend()\n",
    "\n",
    "graph2 = axes[1]\n",
    "graph2.title.set_text('Precision')\n",
    "graph2.plot(lst_prop, lst_precision, color = \"g\",  alpha=1.0, label='Logistic Regression')\n",
    "graph2.plot(lst_prop, lst_precision2, color = \"r\",  alpha=1.0, label='Superquantile Logistic Regression')\n",
    "graph2.fill_between(lst_prop, lst_min_precision, lst_max_precision, color = \"g\", alpha=0.3)\n",
    "graph2.fill_between(lst_prop, lst_min_precision2, lst_max_precision2, color = \"r\", alpha=0.3)\n",
    "graph2.set_xlabel('shift proportion', fontsize=12)\n",
    "graph2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-startup",
   "metadata": {},
   "source": [
    "## 3. The iWildCam Challenge Dataset\n",
    "\n",
    "In this section, we will not introduce any new content, but rather work through the method on a much more realistic example than before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-edward",
   "metadata": {},
   "source": [
    "The [iWildCam](https://wilds.stanford.edu/datasets/#iwildcam) dataset contains images of various elements of nature, such as animals and landscapes. The original number of data points is ~130,000 training examples, ~10,000 validation examples, and ~40,000 test examples. There are a total of 182 training classes, not all of which are present in the validation and test sets. These are captured by camera traps in various wildlife habitats. There is drastic variation in illumination, camera angle, background, vegetation, color, and relative animal frequencies, which results in models generalizing poorly to new camera trap deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-finance",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Print the sorted counts of the classes in the training set. What do you notice? (*Hint:* use `numpy.unique`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_full = np.load(\"data/y_train.npy\")\n",
    "\n",
    "## <YOUR CODE HERE>\n",
    "classes, counts = None\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-assumption",
   "metadata": {},
   "source": [
    "While the original dataset uses images, we have featurized the dataset into lower-dimensional real vectors. Additionally, we have subsampled the data significantly, such that that there are fewer classes, and more data per class. Observe the shapes of the reduced data matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"data/X_train_reduced.npy\")\n",
    "y_train = np.load(\"data/y_train_reduced.npy\")\n",
    "X_val = np.load(\"data/X_val_reduced.npy\")\n",
    "y_val = np.load(\"data/y_val_reduced.npy\")\n",
    "X_test = np.load(\"data/X_test_reduced.npy\")\n",
    "y_test = np.load(\"data/y_test_reduced.npy\")\n",
    "\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "print(\"X val shape:\", X_val.shape)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(np.sort(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-cover",
   "metadata": {},
   "source": [
    "While there this reduced dataset is easier to handle both statistically and computationally, there is significant class imbalance still. We will try downsampling the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-bandwidth",
   "metadata": {},
   "source": [
    "**Exercise 3.2**: Downsample the majority class of `y_train` by a factor of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "## <YOUR CODE HERE>\n",
    "X_tr = None\n",
    "y_tr = None\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-navigation",
   "metadata": {},
   "source": [
    "Here, we store a map from class to index, as not all classes are present in the validation and test set. We also show the performance of the \"chance\" classifier, that predicts the value of the largest class every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = {}\n",
    "for i, class_ in enumerate(classes):\n",
    "    class_to_idx[class_] = i\n",
    "\n",
    "classes, counts = np.unique(y_tr, return_counts=True)\n",
    "print(counts)\n",
    "print(\"Chance:\", counts.max() / counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-complexity",
   "metadata": {},
   "source": [
    "Next, we get baseline performance using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "C_range = np.logspace(-5, 0, 5)\n",
    "\n",
    "best_acc = -1\n",
    "best_C = None\n",
    "lr = None\n",
    "try:\n",
    "    for C in C_range:\n",
    "        print(\"Evaluating C = %0.5f...\" % C)\n",
    "        model = LogisticRegression(C=C, solver='liblinear').fit(X_tr, y_tr)\n",
    "        \n",
    "        acc = accuracy_score(y_val, model.predict(X_val))\n",
    "        print(\"\\t Accuracy:\", acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_C = C\n",
    "            lr = model\n",
    "\n",
    "    print(\"Best C:\", best_C)\n",
    "    print(\"Best accuracy:\", best_acc)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Graceful Exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-trouble",
   "metadata": {},
   "source": [
    "This is significantly better than chance! Now, we will compute a few metrics on the test set. Of interest to us is the performance of a classifier at the *tail of its loss*. We compute both the accuracy and $p$-th superquantile for various values of $p$ on the test set for the logistic regression trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def compute_metrics(y_proba, y_true, ps):\n",
    "    \n",
    "    # Test accuracy.\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Log losses.\n",
    "    n = len(y_true)\n",
    "    losses = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        losses[i] = -np.log(y_proba)[i, class_to_idx[y_true[i]]]\n",
    "\n",
    "    # sq_loss = [sq(losses, p) for p in ps]\n",
    "    sq_loss = [superquantile_function(p, losses) for p in ps]\n",
    "    \n",
    "    return acc, sq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [0.5, 0.75, 0.8, 0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-programming",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_acc, lr_sq_loss = compute_metrics(lr.predict_proba(X_test), y_test, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spqr import DRLogisticRegression\n",
    "\n",
    "def softmax(Z):\n",
    "    np.clip(Z, -10, 10)\n",
    "    norm_constant = np.exp(Z).sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return np.exp(Z) / norm_constant\n",
    "\n",
    "def predict_proba(model, x):\n",
    "    \"\"\" Gives a prediction of x\n",
    "            :param ``numpy.array`` x: input whose label is to predict\n",
    "            :return:  value of the prediction\n",
    "    \"\"\"\n",
    "    self = model\n",
    "    formatted_x = np.ones((x.shape[0], self.n_features + self.fit_intercept))\n",
    "    formatted_x[:, self.fit_intercept:] = x\n",
    "    casted_sol = np.reshape(self.solution, (self.n_features + self.fit_intercept, self.n_classes))\n",
    "    probas = softmax(np.dot(formatted_x, casted_sol))\n",
    "\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-cooler",
   "metadata": {},
   "source": [
    "Next, we apply the the Distributionally Robust Logistic Regression (DRLR) method to the same data, with $p = 0.8$, and a hyperparameter search over $mu$ (an optimization parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = [0.1, 1, 10, 100, 1000]\n",
    "lmbda = 1 / best_C\n",
    "p = 0.8\n",
    "\n",
    "best_mu = -1\n",
    "best_acc = -1\n",
    "try:\n",
    "    for mu in mus:\n",
    "\n",
    "        print(\"Evaluating mu = %f...\" % mu)\n",
    "\n",
    "        model = DRLogisticRegression(p=p, mu=mu, lmbda=lmbda)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        acc = accuracy_score(y_val, model.predict(X_val))\n",
    "        print(\"\\t Accuracy:\", acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_mu = mu\n",
    "            drlr = model\n",
    "\n",
    "    print(\"Best mu:\", best_mu)\n",
    "    print(\"Best accuracy:\", best_acc)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Graceful Exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-adult",
   "metadata": {},
   "source": [
    "Note that the accuracy above is not a good as LR, but it is not what we are optimizing for. Instead, we will observe the tail of the losses, and ensure that the *largest ones are not too large*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "drlr_acc, drlr_sq_loss = compute_metrics(predict_proba(drlr, X_test), y_test, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-project",
   "metadata": {},
   "source": [
    "Finally, plot the $p$-th superquantile of the losses on the test set, as a function of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-cycling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"LR Accuracy:\", lr_acc)\n",
    "print(\"DRLR Accuracy:\", drlr_acc)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "ax.plot(ps, lr_sq_loss, color=\"red\", linewidth=3, label=\"LR\")\n",
    "ax.set_label(\"p\")\n",
    "ax.plot(ps, drlr_sq_loss, color=\"blue\", linewidth=3, label=\"DRLR\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$p$\")\n",
    "ax.set_ylabel(r\"$p$-Superquantile Log-Loss (Test)\")\n",
    "ax.set_title(\"Classification Performance on the iWildCam Benchmark\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-possession",
   "metadata": {},
   "source": [
    "## 4. Clustering with k-means\n",
    "\n",
    "Clustering, also known in signal processing as quantization, is an unsupervised learning method in which observations are associated with an element from a finite class. The elements of this class are considered _cluster labels_, and a _cluster_ consists of all observations with the same label. In this section, we will consider clustering the `Hawks` dataset, which contains physical measurements from three species of hawk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, kmeans_plusplus\n",
    "\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "dat = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Hawks.csv')\n",
    "hawks = dat[['Tail','Wing','Species']].dropna()\n",
    "\n",
    "X = hawks[['Tail', 'Wing']]\n",
    "f, g = pd.factorize(hawks['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=f, marker='o', cmap='Set2', figsize=(6,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-scholarship",
   "metadata": {},
   "source": [
    "A clustering algorithm defines and optimizes a formal criterion for assigning observations to clusters. Here we will consider a classic algorithm: k-means clustering.\n",
    "\n",
    "__k-means__\n",
    "\n",
    "The k-means clustering method partitions observations in a dataset into $k$ clusters by minimizing the sum of within-group variances of each cluster. This criterion expresses the intuitive notion that clusters should represent groups of observations that are \"close\" to one another. Given a set of observations $\\{x_1, ..., x_n\\}$ with each $x_i \\in \\mathbb{R}^d$, the optimization problem is\n",
    "\n",
    "$$\n",
    "\\text{min}_{\\mu_1, ..., \\mu_k} \\sum_{i=1}^n \\text{min}_{\\mu \\in C} ||x_i - \\mu||_2^2\n",
    "$$\n",
    "\n",
    "That is, we search for $k$ cluster centers $\\mu_1, ..., \\mu_k$ such that the sum of squared distances from each observation to the nearest cluster center is minimized. The observation $x_i$ is assigned to the cluster $j \\in \\{1, ..., k\\}$, where $j$ is the index of the cluster center $\\mu_j$ minimizing the distance to $x_i$. In other words,\n",
    "\n",
    "$$\n",
    "x_i \\in C_j \\iff j = \\text{argmin}_{j' \\in \\{1, ..., k\\}} ||x_i - \\mu_{j'}||_2^2,\n",
    "$$\n",
    "\n",
    "where $C_j$ denotes the set of observations in cluster $j$.\n",
    "\n",
    "__Optimization__\n",
    "\n",
    "The optimization problem can be phrased as a search over all assignments of $n$ observations to $k$ clusters. As such, it is combinatorial and non-convex in nature. We define a greedy algorithm that improves the clustering quality (i.e. decreases the objective) at every iteration, and which is guaranteed to converge at a local minimum:\n",
    "\n",
    "- Initialize $k$ cluster centers $\\mu_1, ..., \\mu_k$\n",
    "\n",
    "- While not converged:\n",
    "    - Assign each observation $x_i$ to the cluster $j$ minimizing $||x_i - \\mu_j||_2^2$\n",
    "    - Re-compute each cluster center $\\mu_j$ as the mean of the observations $x_i \\in C_j$:\n",
    "        \n",
    "$$\n",
    "\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\n",
    "$$\n",
    "\n",
    "K-means clustering is implemented in `sklearn` in the `sklearn.cluster` module. Below we introduce the API on a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans in sklearn\n",
    "kmeans = KMeans(n_clusters=3, init='random', \n",
    "                n_init=5, algorithm='full').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_obj(X, centers, labels):\n",
    "    return np.sum([np.linalg.norm(x-centers[labels[i]])**2 for i, x in enumerate(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute for kmeans output\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "kmeans_obj(X.values, centers, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_ # compare to sklearn value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show objective decrease over iterations\n",
    "obj = [KMeans(n_clusters=3, init='random',\n",
    "             n_init=1, algorithm='full',\n",
    "             random_state=0, max_iter=t).fit(X).inertia_ for t in range(1,16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(obj)\n",
    "plt.ylabel('Objective')\n",
    "plt.xlabel('Iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "cmap = cm.get_cmap('Set2')\n",
    "cols = [cmap(x) for x in labels]\n",
    "X['labels'] = labels\n",
    "X.plot.scatter('Wing', 'Tail', c=cols, cmap='Set2')\n",
    "for c in centers:\n",
    "    plt.plot(c[1], c[0], 'kX', markersize=8)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-screw",
   "metadata": {},
   "source": [
    "__The importance of initialization__\n",
    "\n",
    "The non-convexity of the k-means problem implies that the result will depend on the initialization of the algorithm. This can be quite important in practice, with different initializations leading to different cluster assignments, which in turn may vary in their quality (in terms of the objective value) and interpretation. We will consider three approaches to initializing the clusters.\n",
    "\n",
    "1. _Naive method: random initialization_\n",
    "\n",
    "As a first idea, we could consider simply choosing the initial centers at random. While simple to implement, this method completely ignores the data and thus is not likely to work well except in simple and easy cases. \n",
    "\n",
    "A common solution to this problem is to run the entire algorithm multiple times from many random starting points (i.e. \"re-starting\"), and to select the final clustering with the lowest objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_randinit_trial(): # run a single trial with random init\n",
    "    kmeans = KMeans(n_clusters=3, init='random', \n",
    "                    n_init=1, algorithm='full').fit(X)\n",
    "\n",
    "    centers = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    obj = kmeans_obj(X.values, centers, labels)\n",
    "    cols = [cmap(x) for x in labels]\n",
    "    X['labels'] = labels\n",
    "    X.plot.scatter('Wing', 'Tail', c=cols, cmap='Set2')\n",
    "    for c in centers:\n",
    "        plt.plot(c[1], c[0], 'kX', markersize=8)\n",
    "        \n",
    "    plt.title('Final objective value: {:.3f}'.format(obj))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_randinit_trial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-clinic",
   "metadata": {},
   "source": [
    "2. _K-means++_\n",
    "\n",
    "The initialization can often be dramatically improved by using the data to estimate a reasonable starting point. A well-known method for this is the `kmeans++` approach:\n",
    "\n",
    "- Choose an observation uniformly at random from the data. Set this as the first initial center.\n",
    "\n",
    "- While the number of centers chosen is less than k:\n",
    "    - For each $x_j$ that is not a center, compute\n",
    "    \n",
    "    $$\n",
    "    d_j = \\text{min}_{c \\in C^{(0)}} ||x_j - c||_2^2,\n",
    "    $$\n",
    "    where $C^{(0)}$ is the set of initial centers already chosen.\n",
    "    \n",
    "    - Select a new observation as an additional center by drawing randomly from the set $\\{x_j : x_j \\notin C^{(0)}$ with probability proportional to $d_j^2$.\n",
    "- Return set of initial centers $C^{(0)}$.\n",
    "\n",
    "This is implemented as an optional argument to the `KMeans` constructor, but we can also use the function `kmeans_plusplus` to directly compute and visualize the results of this initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpp_centers, kpp_ixs = kmeans_plusplus(X.values, n_clusters=3)\n",
    "kmeans = KMeans(n_clusters=3, init=kpp_centers, n_init=1, algorithm='full').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "X.plot.scatter('Wing', 'Tail', ax=axs[0])\n",
    "for c in kpp_centers:\n",
    "    axs[0].plot(c[1], c[0], 'rX', markersize=8)\n",
    "axs[0].set_title('Kmeans++ initialization')    \n",
    "\n",
    "cmap = cm.get_cmap('Set2')\n",
    "cols = [cmap(x) for x in kmeans.labels_]\n",
    "X.plot.scatter('Wing', 'Tail', ax=axs[1], c=cols)\n",
    "for c in kmeans.cluster_centers_:\n",
    "    axs[1].plot(c[1], c[0], 'kX', markersize=8)\n",
    "axs[1].set_title('Final objective value: {:.3f}'.format(kmeans.inertia_));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-saver",
   "metadata": {},
   "source": [
    "3. _Iterative Voronoi centroids_\n",
    "\n",
    "A third option, developed in the signal processing literature for quantization, iteratively constructs the initial cluster centers by the following algorithm:\n",
    "\n",
    "- Start with a single center, computed as the mean over all the data.\n",
    "- While the number of centers chosen is less than k:\n",
    "    - For each cluster chosen, create two new ones by adding and subtracting $\\delta$\n",
    "    - Partition the observations according to which of these new centers is the closest\n",
    "    - Compute new cluster centers by taking the mean of all observations within each partition\n",
    "- Return the set of initial centers.\n",
    "\n",
    "This procedure doubles the number of initial centers at each iteration, so is only a valid initialization procedure when $k = 2^j$ for some $j \\in \\mathbb{N}$. We'll implement and visualize it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name comes from algorithm inventors\n",
    "def LBG_initialization(X, k, delta=0.1):\n",
    "    if np.log2(k)%1 != 0:\n",
    "        raise ValueError('k must be a power of 2')\n",
    "        \n",
    "    C = np.mean(X, axis=0)[None, :]\n",
    "    nk, d = C.shape\n",
    "    while nk < k:\n",
    "        Cnew = np.zeros((2*nk, d))\n",
    "        for j in range(nk):\n",
    "            Cnew[2*j, :] = C[j, :]-delta\n",
    "            Cnew[(2*j)+1, :] = C[j, :]+delta\n",
    "            \n",
    "        C = compute_centroids(X, Cnew)\n",
    "        nk, d = C.shape\n",
    "    return C\n",
    "\n",
    "def compute_centroids(X, V):\n",
    "    '''\n",
    "    Return the centroids of the Voronoi partition induced by vectors in V.\n",
    "    '''\n",
    "    dists = np.hstack([np.linalg.norm(X-V[k][None, :], axis=1)[:, None] for \n",
    "                       k in range(V.shape[0])])\n",
    "    labs = np.argmin(dists, axis=1)\n",
    "    \n",
    "    C = np.zeros(V.shape)\n",
    "    for i in range(C.shape[0]):\n",
    "        C[i] = np.mean(X[labs==i,:], axis=0)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out\n",
    "k = 4\n",
    "lbg_init = LBG_initialization(X.values, k, delta=5)\n",
    "kmeans_lbg = KMeans(n_clusters=k, init=lbg_init, n_init=1, algorithm='full').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "X.plot.scatter('Wing', 'Tail', ax=axs[0])\n",
    "for c in lbg_init:\n",
    "    axs[0].plot(c[1], c[0], 'rX', markersize=8)\n",
    "axs[0].set_title('LBG initialization')    \n",
    "\n",
    "cmap = cm.get_cmap('Set2')\n",
    "cols = [cmap(x) for x in kmeans_lbg.labels_]\n",
    "X.plot.scatter('Wing', 'Tail', ax=axs[1], c=cols)\n",
    "for c in kmeans_lbg.cluster_centers_:\n",
    "    axs[1].plot(c[1], c[0], 'kX', markersize=8)\n",
    "axs[1].set_title('Final objective value: {:.3f}'.format(kmeans_lbg.inertia_));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-virgin",
   "metadata": {},
   "source": [
    "__Choosing k__\n",
    "\n",
    "The number of clusters $k$ is an algorithm hyperparamter. There are a wide range of proposals for how this can be selected in practice. A classical perspective is to compare results over a range of choices of $k$ and look for an \"elbow\" in the plot of objective value versus $k$. This indicates a value of $k$ past which increasing the number of clusters does not appear to provide much further benefit in terms of reducing the objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = np.arange(1,11)\n",
    "obj = [KMeans(n_clusters=k, init='k-means++',\n",
    "             n_init=10, algorithm='full',\n",
    "             random_state=0).fit(X).inertia_ for k in k_range]\n",
    "\n",
    "plt.plot(k_range, obj)\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel('Objective');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-permission",
   "metadata": {},
   "source": [
    "Perhaps a more well-rounded perspective is to understand that clustering almost always occurs in the context of some analytical pipeline with a downstream objective. It is rarely the case - and discouraged in general - that we invest much effort in interpreting the clusters themselves; rather, we adopt a pragmatic perspective that asks whether clustering can improve an analysis with respect to some metric of interest - e.g. accuracy or computational cost. Under this view, the number of clusters becomes a method hyperparameter that we can validate over in the usual sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-cooler",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
